# -*- coding: utf-8 -*-
"""Positional_Index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BtrCJN93HXI5alkfK5msCNq_hE5Z0tLL
"""

import os
import pickle
import codecs
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

root = "processed/"

file_list = []
for path, subdirs, files in os.walk(root):
    for name in files:
        file_list.append(os.path.join(path, name))

# file_map = {}
# fileno=1
#index = {}
# for file in file_list:

#     filename = file.split('/')[-1]
#     file_map[fileno] = filename
#     fileno = fileno+1


# for file in file_list:

#     filename = file.split('/')[-1]
    # text = codecs.open(file,'r','unicode_escape')
    # filetext = text.read()
    # words = filetext.split(' ')
    
#     for pos,word in enumerate(words):
#         if word in index:
#             index[word][0] = index[word][0] + 1
            
#             if fileno in index[word][1]:
#                 index[word][1][fileno].append(pos)
                              
#             else:
#                 index[word][1][fileno] = [pos]
#         else:
#             index[word] = []
#             index[word].append(1)
#             index[word].append({})      
#             index[word][1][fileno] = [pos]
            
#     print(fileno)
#     file_map[fileno] = filename
#     fileno=fileno+1

# opfile = open('pos_index_final.pkl','wb')
# pickle.dump(index,opfile)
# opfile.close()

# opfile = open('file_map.pkl','wb')
# pickle.dump(file_map,opfile)
# opfile.close()

#print('Created files')

#print('lol')
index = pickle.load(open("pos_index_final.pkl","rb"))

file_map = pickle.load(open("file_map.pkl","rb"))
#print(index)

def preprocess(filetext):
        filetext = filetext.lower()

        #Punctuation removal
        filetext = re.split("[" + string.punctuation + "]+", filetext)
        filetext = " ".join(filetext)

        #Tokenization
        words = word_tokenize(filetext)

        #Stopword removal
        stop_words = set(stopwords.words('english'))
        filtered = [word for word in words if not word in stop_words]
        for i in range(len(filtered)):
            filtered[i] = filtered[i].strip()
        print(filtered)
        
        return filtered

def get_doclist(file_map,mainlist):

    klist = list(file_map.keys())
    vlist = list(file_map.values())
    result = []

    for i in range(len(mainlist)):

        posi = klist.index(mainlist[i])
        result.append(vlist[posi])

    return result

# def Query_result(words):
#     mainset = set(index.get(words[0])[1])
    
#     for i in range(1,len(words)):
#         mainset = mainset.intersection(index.get(words[i])[1])
        
#     print("Number of documents matched: ", len(mainset))
#     print("List of documents matched: ")
#     print(get_doclist(file_map,list(mainset)))

def getQueryDocs(position_list,words):
    result = []
    n = len(words)-1
    
    for x in range(len(position_list)-n):
        if position_list[x+n][1]-position_list[x][1] == n:
            if position_list[x+n][0] == position_list[x][0]:
                #print ("check")
                true_count=0
                for i in range(len(words)):
                    if position_list[x+i][2] == words[i]:
                        true_count = true_count+1
                
                #print(true_count)
                if true_count==len(words):
                    result.append(position_list[x][0])
                    #print("Position at doc ",position_list[x][0]," is ", position_list[x+n][1] ," and ", position_list[x][1])
    
    return list(set(result))

def Query_result2(words):
    mainset = set(index.get(words[0])[1])
    
    for i in range(1,len(words)):
        mainset = mainset.intersection(index.get(words[i])[1])
        
    doc_list = list(mainset)
    position_list = []

    for d in doc_list:
        for w in range(len(words)):
            for x in index[words[w]][1][d]:
                position_list.append([d,x,words[w]])
        
    position_list = sorted(position_list, key=lambda x: (x[0],x[1]))
    final_doc_list = getQueryDocs(position_list,words)
    
    print("Number of documents matched: ", len(final_doc_list))
    print("List of documents matched: ")
    print(get_doclist(file_map,final_doc_list))

print('Give input')
words = input()
words = preprocess(words)

Query_result2(words)

