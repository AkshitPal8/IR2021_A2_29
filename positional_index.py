# -*- coding: utf-8 -*-
"""Positional_Index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BtrCJN93HXI5alkfK5msCNq_hE5Z0tLL
"""

import os
import pickle
import codecs
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

root = "processed/"

file_list = []
for path, subdirs, files in os.walk(root):
    for name in files:
        file_list.append(os.path.join(path, name))

# file_map = {}
# fileno=1
#index = {}
# for file in file_list:

#     filename = file.split('/')[-1]
#     file_map[fileno] = filename
#     fileno = fileno+1


# for file in file_list:

#     filename = file.split('/')[-1]
#     text = codecs.open(file,'r','unicode_escape')
#     filetext = text.read()
#     words = filetext.split(' ')
    
#     for pos,word in enumerate(words):
#         if word in index:
#             index[word][0] = index[word][0] + 1
            
#             if fileno in index[word][1]:
#                 index[word][1][fileno].append(pos)
                              
#             else:
#                 index[word][1][fileno] = [pos]
#         else:
#             index[word] = []
#             index[word].append(1)
#             index[word].append({})      
#             index[word][1][fileno] = [pos]
            
#     print(fileno)
#     file_map[fileno] = filename
#     fileno=fileno+1

# opfile = open('pos_index_final.pkl','wb')
# pickle.dump(index,opfile)
# opfile.close()

# opfile = open('file_map.pkl','wb')
# pickle.dump(file_map,opfile)
# opfile.close()

#print('Created files')

#print('lol')
index = pickle.load(open("pos_index_final.pkl","rb"))

file_map = pickle.load(open("file_map.pkl","rb"))
#print(index)

def preprocess(filetext):
        filetext = filetext.lower()

        #Punctuation removal
        filetext = re.split("[" + string.punctuation + "]+", filetext)
        filetext = " ".join(filetext)

        #Tokenization
        words = word_tokenize(filetext)

        #Stopword removal
        stop_words = set(stopwords.words('english'))
        filtered = [word for word in words if not word in stop_words]
        for i in range(len(filtered)):
            filtered[i] = filtered[i].strip()
        print(filtered)
        
        return filtered

def get_doclist(file_map,mainlist):

    klist = list(file_map.keys())
    vlist = list(file_map.values())
    result = []

    for i in range(len(mainlist)):

        posi = klist.index(mainlist[i])
        result.append(vlist[posi])

    return result

def Query_result(words):
    mainset = set(index.get(words[0])[1])
    
    for i in range(1,len(words)):
        mainset = mainset.intersection(index.get(words[i])[1])
        
    print("Number of documents matched: ", len(mainset))
    print("List of documents matched: ")
    print(get_doclist(file_map,list(mainset)))

print('Give input')
words = input()
words = preprocess(words)

Query_result(words)

