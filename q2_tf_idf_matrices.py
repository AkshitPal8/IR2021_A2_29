# -*- coding: utf-8 -*-
"""Q2 TF-IDF Matrices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sZiwVhvNO0Nb-9-RXvutDCb4uldXK2M-
"""

import os
import pickle
import codecs
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import math

"""Navigate to working directory"""

cd '/content/drive/MyDrive/College/Semester 8/IR/Assignment 2'

"""Unpickle the positional index and file map"""

index = pickle.load(open("pos_index_final.pkl","rb"))
file_map = pickle.load(open("file_map.pkl","rb"))

#succesfully loaded, this has been tested

"""Make a vocabulary map first"""

word_ID = 1    #the words will be following one-based indexing as the files have followed the same
vocab_map = {}

wordlist = index.keys()

for word in wordlist:

  vocab_map[word_ID] = word
  word_ID += 1

#vocab_map made successfully - tested

"""Pickle vocabulary map as "word_map""""

opfile = open('word_map.pkl','wb')
pickle.dump(vocab_map,opfile)
opfile.close()

#DON'T RUN THIS CELL AGAIN

"""At the moment, word_ID - 1 = vocabsize, thus we save it as such. This will be useful when **computing TF-IDF scores for a query**"""

vocab_size = word_ID - 1

"""We also need to make an inverse word map to handle the query"""

inv_vocab_map = {}

for i in vocab_map.keys():

  inv_vocab_map[vocab_map[i]] = i

"""We should pickle this as well"""

#PICKLE inv_vocab_map
opfile = open('inverse_word_map.pkl','wb')
pickle.dump(inv_vocab_map,opfile)
opfile.close()

print(inv_vocab_map.keys())
print(inv_vocab_map.values())

"""Similarly, making an inverse file map may help us later. Pickle this too"""

#MAKE inv_file_map

#PICKLE the inverse file map also

"""Function to compute idf score given word_ID"""

def IDF(word_ID):

  if word_ID not in vocab_map:
    return 0

  word = vocab_map[word_ID]
  df = len(index[word][1])

  idf = len(file_map.keys())/df   # total number of documents / number of documents containing given word
  idf += 1
  idf = math.log(idf) #if required, change this to math.log(idf, base=10)  

  return idf

"""Functions to compute tf score of a given (file_ID, word_ID) pair using 

1.   Binary
2.   Raw count
3.   term frequency
4.   log normalization
5.   double normalization


"""

def tf_binary(file_ID, word_ID):

  if word_ID not in vocab_map:
    return 0

  word = vocab_map[word_ID]

  if file_ID in index[word][1]:

    return 1

  else:

    return 0

def tf_raw_count(file_ID, word_ID):

  if word_ID not in vocab_map:
    return 0

  word = vocab_map[word_ID]

  if file_ID in index[word][1]:

    tf = len(index[word][1][file_ID])
    return tf

  else:

    return 0

"""In **TF Term Frequency** want to avoid summing over all words everytime so we precompute this for all documents."""

term_freq_helper = [0]*(len(file_map.keys()) + 1)
# precompute Sum(frquency(word', fileno)) for all documents

for file_ID in file_map.keys():

  for word in inv_vocab_map.keys():

    if file_ID in index[word][1]:

      term_freq_helper[file_ID] += len(index[word][1][file_ID])

#the sums have been precomputed

def tf_term_frequency(file_ID, word_ID):

  if word_ID not in vocab_map:
    return 0

  word = vocab_map[word_ID]

  if file_ID in index[word][1]:

    tf = len(index[word][1][file_ID])

    #Sum of term frequencies of all words in document
    tfsum = term_freq_helper[file_ID]

    tf = tf / tfsum
    return tf

  else:

    return 0

def tf_log_normalization(file_ID, word_ID):

  if word_ID not in vocab_map:
    return 0

  word = vocab_map[word_ID]

  if file_ID in index[word][1]:

    tf = len(index[word][1][file_ID]) + 1
    tf = math.log(tf)   #if required, change this to math.log(tf, base = 10)
    return tf

  else:

    return 0

"""For **Double Normalization TF** we need a similar helper list to avoid excess computation"""

double_normalization_helper = [0]*(len(file_map.keys()) + 1)
# precompute max(frquency(word', fileno)) for all documents

for file_ID in file_map.keys():

  cur = -1

  for word in inv_vocab_map.keys():

    if file_ID in index[word][1]:
      
      cur = max(cur, len(index[word][1][file_ID]))
    
  double_normalization_helper[file_ID] = cur

#the maximums have been precomputed

def tf_double_normalization(file_ID, word_ID):

  if word_ID not in vocab_map:
    return 0.5

  word = vocab_map[word_ID]

  if file_ID in index[word][1]:

    tf = len(index[word][1][file_ID])

    #to compute maximum of term frequencies of all words in document
    maximum = double_normalization_helper[file_ID]

    tf = 0.5*(tf/maximum)
    tf += 0.5

    return tf

  else:

    return 0.5

"""Now that the functions are made, we need to make and pickle the tf idf tables using all of the given tf metrics.

We choose to make each table as a **2D List** as this will help us easily get tf-idf vectors for individual documents when we need them for cosine similarity computations
"""

num_files = len(file_map.keys())
num_words = vocab_size

"""TF-IDF Matrix using Binary TF - **DONE**"""

Mat_Binary = [[0]*(num_words + 1)]*(num_files+1)

for j in range(1, num_words+1):

  idf_j = IDF(j)

  for i in range(1, num_files+1):
    Mat_Binary[i][j] = tf_binary(i, j)*idf_j

#Pickle the above matrix

opfile = open('Binary_TF-IDF_Matrix.pkl','wb')
pickle.dump(Mat_Binary,opfile)
opfile.close()

"""TF-IDF Matrix using Raw Count TF - **DONE**"""

Mat_raw_count = [[0]*(num_words + 1)]*(num_files+1)

for j in range(1, num_words+1):

  idf_j = IDF(j)

  for i in range(1, num_files+1):
    Mat_raw_count[i][j] = tf_raw_count(i, j)*idf_j

#Pickle the above matrix

opfile = open('Raw_Count_TF-IDF_Matrix.pkl','wb')
pickle.dump(Mat_raw_count,opfile)
opfile.close()

"""TF-IDF Matrix using Term Frequency TF - **DONE**"""

Mat_term_frequency = [[0]*(num_words + 1)]*(num_files+1)

for j in range(1, num_words+1):

  idf_j = IDF(j)

  for i in range(1, num_files+1):
    Mat_term_frequency[i][j] = tf_term_frequency(i, j)*idf_j

#Pickle the above matrix

opfile = open('Term_Frequency_TF-IDF_Matrix.pkl','wb')
pickle.dump(Mat_term_frequency,opfile)
opfile.close()

"""TF-IDF Matrix using Log normalization TF - **DONE**"""

Mat_log_normalization = [[0]*(num_words + 1)]*(num_files+1)

for j in range(1, num_words+1):

  idf_j = IDF(j)

  for i in range(1, num_files+1):
    Mat_log_normalization[i][j] = tf_log_normalization(i, j)*idf_j

#Pickle the above matrix

opfile = open('Log_Normalization_TF-IDF_Matrix.pkl','wb')
pickle.dump(Mat_log_normalization,opfile)
opfile.close()

"""TF-IDF Matrix using Double Normalization TF - **DONE**"""

Mat_double_normalization = [[0]*(num_words + 1)]*(num_files+1)

for j in range(1, num_words+1):

  idf_j = IDF(j)

  for i in range(1, num_files+1):
    Mat_double_normalization[i][j] = tf_double_normalization(i, j)*idf_j

#Pickle the above matrix

opfile = open('Double_Normalization_TF-IDF_Matrix.pkl','wb')
pickle.dump(Mat_double_normalization,opfile)
opfile.close()