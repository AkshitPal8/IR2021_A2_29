# -*- coding: utf-8 -*-
"""IR_ASS_2_Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IwRVNm8OZvQtBQWOqPghwjyEk3R4c_2z
"""

with open('/content/drive/MyDrive/IR-assignment-2-data.txt', 'r') as f:
  lines = f.readlines()

"""# Part 2"""

relevance_scores = []

for line in lines:
  c     = line.split()
  a, r  = int(c[0]), c[1]
  if r!="qid:4":continue
  relevance_scores.append(a)

relevance_scores.sort(reverse=True)

"""Directly using relevance scores here."""

print(relevance_scores)

"""Tabulating count for each relevance score."""

rel_dict = {}
for i in relevance_scores:
  if i not in rel_dict:
    rel_dict[i] = 0
  rel_dict[i] += 1

rel_dict

"""We can re-rank each document with the same relevance score such that the maximum DCG remains the same. In this case, the total number of ways would be $59!\times26!\times17!\times1$, which would also be the number of unique files made."""

dcg = relevance_scores[0]
for i in range(len(relevance_scores)):
  if relevance_scores[i]==dcg:continue                # Skip the initial value, as it has already been considered.
  dcg += relevance_scores[i]/(np.log(i+1)/np.log(2))  # Follows 1-indexing, adjust accordingly. 

print("Max DCG:", dcg)

"""Max DCG: 20.989750804831445

Writing new ranking for $qid:4$ to file:
"""

all_lines = []

for line in lines:
  c = line.split()
  r = c[1]
  if r!="qid:4":continue
  all_lines.append(c)

all_lines.sort(reverse=True, key=lambda x: int(x[0]))

with open('/content/qid_4_max_dcg.txt', 'w') as f:
  for line in all_lines:
    f.write(' '.join(line)+'\n')

"""# Part 3

We use the idealized ranking from the previous part to calculate the DCG at rank $n$.
"""

relevance_scores  = []
ideal_scores      = []

for line in lines:
  c     = line.split()
  a, r  = int(c[0]), c[1]
  if r!="qid:4":continue
  relevance_scores.append(a)
  ideal_scores.append(a)

ideal_scores.sort(reverse=True)

print(relevance_scores)
print(ideal_scores)

"""## Part 3a."""

dcg_50  = relevance_scores[0]
idcg_50 = ideal_scores[0]

for i in range(50):
  if relevance_scores[i]==dcg_50:continue                # Skip the initial value, as it has already been considered.
  dcg_50 += relevance_scores[i]/(np.log(i+1)/np.log(2))  # Follows 1-indexing, adjust accordingly. 

for i in range(50):
  if ideal_scores[i]==idcg_50:continue                # Skip the initial value, as it has already been considered.
  idcg_50 += ideal_scores[i]/(np.log(i+1)/np.log(2))  # Follows 1-indexing, adjust accordingly. 

print("NDCG@50:", dcg_50/idcg_50)

"""NDCG@50: 0.3521042740324887

## Part 3b.
"""

dcg   = relevance_scores[0]
idcg  = ideal_scores[0]

for i in range(len(relevance_scores)):
  if relevance_scores[i]==dcg:continue                # Skip the initial value, as it has already been considered.
  dcg += relevance_scores[i]/(np.log(i+1)/np.log(2))  # Follows 1-indexing, adjust accordingly. 

for i in range(len(ideal_scores)):
  if ideal_scores[i]==idcg:continue                # Skip the initial value, as it has already been considered.
  idcg += ideal_scores[i]/(np.log(i+1)/np.log(2))  # Follows 1-indexing, adjust accordingly. 

print("NDCG of full dataset:", dcg/idcg)

"""NDCG for the whole dataset: 0.5979226516897831

# Part 4

Relevance values belong to $[0,4]$, but we will consider anything above and equal to $1$ as a positive sample.
"""

scores  = []              # Tf-idf score list.
max_val = -float('inf')   # For normalizing scores.
min_val = float('inf')    # For normalizing scores.
gt      = []              # Ground truth relevance prediction, as per given file.
pred    = []              # Predicted relevance, {0,1}.

for line in lines:
  c       = line.split()
  a, r, b = int(c[0]), c[1], float(c[76][3:])

  if r!="qid:4":continue
  if b >= max_val: max_val = b
  if b < min_val: min_val = b

  if a==0:
    gt.append(0)
  else:
    gt.append(1)
  
  scores.append(b)

for score in scores:
  a = (score - min_val)/(max_val - min_val)   # Normalizing scores.
  pred.append(a)

import numpy as np

precision_scores = []
recall_scores = []
thresholds = np.linspace(0, 1, 100)           # Thresholding.

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

for t in thresholds:
  temp = []
  for p in pred:
    if p >= t:
      temp.append(1)
    else:
      temp.append(0)

  precision_scores.append(precision_score(gt, temp))  # Calculating precision
  recall_scores.append(recall_score(gt, temp))        # Calculating recall

"""Plotting precision-recall curve."""

import matplotlib.pyplot as plt
plt.plot(recall_scores, precision_scores)
plt.title("Precision-Recall curve")
plt.ylabel("Precision")
plt.xlabel("Recall")
plt.show()