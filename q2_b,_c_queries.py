# -*- coding: utf-8 -*-
"""Q2 b, c - Queries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L-Sihvj-9Zjd7lfzkygD1ZFtou0evZMm
"""

import os
import pickle
import codecs
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import math

from numpy import dot
from numpy.linalg import norm

#cd '/content/drive/MyDrive/College/Semester 8/IR/Assignment 2'

#loading all the pickled data items

index = pickle.load(open("pos_index_final.pkl","rb"))
file_map = pickle.load(open("file_map.pkl","rb"))
word_map = pickle.load(open("word_map.pkl","rb"))
inverse_word_map = pickle.load(open("inverse_word_map.pkl","rb"))

TFIDF_binary = pickle.load(open("Binary_TF-IDF_Matrix.pkl","rb"))
TFIDF_raw_count = pickle.load(open("Raw_Count_TF-IDF_Matrix.pkl","rb"))
TFIDF_term_frequency = pickle.load(open("Term_Frequency_TF-IDF_Matrix.pkl","rb"))
TFIDF_log_normalization = pickle.load(open("Log_Normalization_TF-IDF_Matrix.pkl","rb"))
TFIDF_double_normalization = pickle.load(open("Double_Normalization_TF-IDF_Matrix.pkl","rb"))

print(TFIDF_log_normalization[0])


def IDF(word_ID):

  if word_ID not in word_map:
    return 0

  word = word_map[word_ID]
  df = len(index[word][1])

  idf = len(file_map.keys())/df   # total number of documents / number of documents containing given word
  idf += 1
  idf = math.log(idf) #if required, change this to math.log(idf, base=10)  

  return idf



"""Now that the basic query vector with raw term counts has been made, we will use this to get tf-idf scores of documents using all the weighting schemes

**2 - b - parts 5/6**

Functions for getting top 5 most relevant documents using different weighting schemes
"""

# Gets the names of top 5 most relevant docs acc to Binary TFIDF weighting

def Top_5_binary(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    score = 0
    doc_name = file_map[doc_ID]

    for word_ID in range(1, len(word_map.keys()) + 1):

      if (query_vector[word_ID] > 0):

        score += TFIDF_binary[doc_ID][word_ID]
    
    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

  return Top_Docs

# Gets the names of top 5 most relevant docs acc to Raw Count TFIDF weighting

def Top_5_raw_count(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    score = 0
    doc_name = file_map[doc_ID]

    for word_ID in range(1, len(word_map.keys()) + 1):

      if (query_vector[word_ID] > 0):

        score += TFIDF_raw_count[doc_ID][word_ID]
    
    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

  return Top_Docs

# Gets the names of top 5 most relevant docs acc to Term Frequency TFIDF weighting

def Top_5_term_frequency(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    score = 0
    doc_name = file_map[doc_ID]

    for word_ID in range(1, len(word_map.keys()) + 1):

      if (query_vector[word_ID] > 0):

        score += TFIDF_term_frequency[doc_ID][word_ID]
    
    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# Gets the names of top 5 most relevant docs acc to log normalization TFIDF weighting

def Top_5_log_normalization(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    score = 0
    doc_name = file_map[doc_ID]

    for word_ID in range(1, len(word_map.keys()) + 1):

      if (query_vector[word_ID] > 0):

        score += TFIDF_log_normalization[doc_ID][word_ID]
    
    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# Gets the names of top 5 most relevant docs acc to double normalization TFIDF weighting

def Top_5_double_normalization(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    score = 0
    doc_name = file_map[doc_ID]

    for word_ID in range(1, len(word_map.keys()) + 1):

      if (query_vector[word_ID] > 0):

        score += TFIDF_double_normalization[doc_ID][word_ID]
    
    doc_scores[doc_name] = score
  
  print(doc_scores)
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

  return Top_Docs

"""**2 - c**

Functions to get TFIDF vectors of the query according to different weighting schemes
"""

#make query TFIDF vectors for each weighting scheme

#function for TFIDF vector - Binary weighting scheme

def binary_TF(query_vector):

  query_tfidf_vec_Binary = [0]*(len(word_map.keys()) + 1)

  for word_ID in word_map.keys():

    if (query_vector[word_ID] > 0):

        tf = 1
        idf = IDF(word_ID)
        query_tfidf_vec_Binary[word_ID] = tf*idf

  return query_tfidf_vec_Binary

#function for TFIDF vector - raw count scheme

def raw_count_TF(query_vector):

  query_tfidf_vec_raw_count = [0]*(len(word_map.keys()) + 1)

  for word_ID in word_map.keys():

    if (query_vector[word_ID] > 0):

        tf = query_vector[word_ID]
        idf = IDF(word_ID)
        query_tfidf_vec_raw_count[word_ID] = tf*idf

  return query_tfidf_vec_raw_count

#function for TFIDF vector - term frequency scheme

def term_frequency_TF(query_vector):

  query_tfidf_vec_term_frequency = [0]*(len(word_map.keys()) + 1)

  for word_ID in word_map.keys():

    if (query_vector[word_ID] > 0):

        tf = query_vector[word_ID]/sum_freqs
        idf = IDF(word_ID)
        query_tfidf_vec_term_frequency[word_ID] = tf*idf

  return query_tfidf_vec_term_frequency

#function for TFIDF vector - log normalization scheme

def log_normalization_TF(query_vector):

  query_tfidf_vec_log_normalization = [0]*(len(word_map.keys()) + 1)

  for word_ID in word_map.keys():

    if (query_vector[word_ID] > 0):

        tf = math.log(1 + query_vector[word_ID])

        idf = IDF(word_ID)
        query_tfidf_vec_log_normalization[word_ID] = tf*idf

  return query_tfidf_vec_log_normalization

#function for TFIDF vector - double normalization scheme

def double_normalization_TF(query_vector):

  query_tfidf_vec_double_normalization = [0]*(len(word_map.keys()) + 1)

  for word_ID in word_map.keys():

    if (query_vector[word_ID] > 0):

        tf = 0.5 + 0.5*(query_vector[word_ID]/max_freq)

        idf = IDF(word_ID)
        query_tfidf_vec_double_normalization[word_ID] = tf*idf

    else:

        query_tfidf_vec_double_normalization[word_ID] = 0.5

  return query_tfidf_vec_double_normalization

"""Use the above functions to get query tf-idf vectors for use in cosine similarity calculations

**Please kardiyo ye part bhot kam code likhna h yr**
"""

# Cosine Similarity Function

def cosine_sim(doc_vec_TFIDF, query_vec_TFIDF):

  cos_sim = dot(doc_vec_TFIDF, query_vec_TFIDF)/(norm(doc_vec_TFIDF)*norm(query_vec_TFIDF))

  return cos_sim

  #no processing required for the input vectors, just apply cosine similarity

"""Top 5 based on Cosine similarity - calculation functions"""

# USES COSINE SIMILARITY to get the names of top 5 most relevant docs acc to binary TFIDF weighting

def Top_5_binary_cosine(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    doc_name = file_map[doc_ID]

    doc_vec_TFIDF = TFIDF_binary[doc_ID]
    query_vec_TFIDF = binary_TF(query_vector)

    score = cosine_sim(doc_vec_TFIDF, query_vec_TFIDF)

    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# USES COSINE SIMILARITY to get the names of top 5 most relevant docs acc to raw count TFIDF weighting

def Top_5_raw_count_cosine(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    doc_name = file_map[doc_ID]

    doc_vec_TFIDF = TFIDF_raw_count[doc_ID]
    query_vec_TFIDF = raw_count_TF(query_vector)

    score = cosine_sim(doc_vec_TFIDF, query_vec_TFIDF)

    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# USES COSINE SIMILARITY to get the names of top 5 most relevant docs acc to term frequency TFIDF weighting

def Top_5_term_frequency_cosine(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    doc_name = file_map[doc_ID]

    doc_vec_TFIDF = TFIDF_term_frequency[doc_ID]
    query_vec_TFIDF = term_frequency_TF(query_vector)

    score = cosine_sim(doc_vec_TFIDF, query_vec_TFIDF)

    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# USES COSINE SIMILARITY to get the names of top 5 most relevant docs acc to log normalization TFIDF weighting

def Top_5_log_normalization_cosine(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    doc_name = file_map[doc_ID]

    doc_vec_TFIDF = TFIDF_log_normalization[doc_ID]
    query_vec_TFIDF = log_normalization_TF(query_vector)

    score = cosine_sim(doc_vec_TFIDF, query_vec_TFIDF)

    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5

  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

# USES COSINE SIMILARITY to get the names of top 5 most relevant docs acc to double normalization TFIDF weighting

def Top_5_double_normalization_cosine(query_vector):

  doc_scores = {}   #dictionary to hold document names and their respective scores

  for doc_ID in file_map:

    doc_name = file_map[doc_ID]

    doc_vec_TFIDF = TFIDF_double_normalization[doc_ID]
    query_vec_TFIDF = double_normalization_TF(query_vector)

    score = cosine_sim(doc_vec_TFIDF, query_vec_TFIDF)

    doc_scores[doc_name] = score
  
  Top_Docs = []
  count = 5
  print(doc_scores)
  while (count):

    Keymax = max(doc_scores, key=doc_scores.get)
    Top_Docs.append(Keymax)
    doc_scores[Keymax] = -doc_scores[Keymax]

    count -= 1

  return Top_Docs



#SPACE TO LOAD THE QUERY - I've written the rest assuming its a string named 'query'
query = input()  #for now

words = query.split(' ')
query_vector = [0]*(len(word_map.keys()) + 1)
#query_vector will hold the raw term frequencies of all the terms that occur in it 

sum_freqs = 0   #this will help with tf calculation with term frequency method
max_freq = 0    #this will help with tf calculation with double normalization method

for word in words:

  if word in inverse_word_map:
    print('Exist')
    word_ID = inverse_word_map[word]
    query_vector[word_ID] += 1

    sum_freqs += 1
    max_freq = max(max_freq, query_vector[word_ID])

#print(query_vector)

#print(Top_5_double_normalization_cosine(query_vector))
